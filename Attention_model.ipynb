{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b812a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,LSTM,Dense,Bidirectional,Concatenate,Attention\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9628e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'fra-eng/fra.txt'\n",
    "latent_dim = 256 # Content vector / Hyper-parameter tunne \n",
    "\n",
    "num_sample = 10000 # use first 10000 samples from the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cdc0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data into list \n",
    "# input_words # translate from\n",
    "# output_words # translate to\n",
    "\n",
    "input_texts = []\n",
    "output_texts = []\n",
    "\n",
    "# Unique input character (a,b,c,d ......)\n",
    "# unique output character(.....)\n",
    "# use build in data type set for non unique char\n",
    "\n",
    "input_char = set()\n",
    "output_char = set()\n",
    "\n",
    "\n",
    "# opeining the file \n",
    "\n",
    "with open(data_path,'r',encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "# num_sample take the first 10000 rows to train \n",
    "\n",
    "for line in lines[:num_sample]:\n",
    "    input_word, output_word,_ = line.split(\"\\t\")\n",
    "\n",
    "    # Adding Start and End to the target word so that the decoder knows where to start from and when to stop\n",
    "    \n",
    "    output_word = 'Start' + output_word + 'End'\n",
    "    \n",
    "    # appending the input_word word and output_word to the list\n",
    "    \n",
    "    input_texts.append(input_word)\n",
    "    output_texts.append(output_word)\n",
    "    \n",
    "    # adding the unique character into the set\n",
    "    \n",
    "    for char in input_word:\n",
    "        if char not in input_char:\n",
    "            input_char.add(char)\n",
    "    for char in output_word:\n",
    "        if char not in output_char:\n",
    "            output_char.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa1d7031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the unique char in ascending order\n",
    "\n",
    "input_char = sorted(input_char)\n",
    "output_char = sorted(output_char)\n",
    "\n",
    "# encoder/decoder length \n",
    "# unique char-length\n",
    "\n",
    "encoder_length = len(input_char)\n",
    "decoder_length = len(output_char)\n",
    "\n",
    "# Finding the maximum length of sentence\n",
    "\n",
    "max_encoder_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_length = max([len(txt) for txt in output_texts])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e74fd222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign tokens to unique char\n",
    "\n",
    "input_token = dict([(char,i) for i,char in enumerate(input_char)])\n",
    "output_token = dict([(char,i) for i ,char in enumerate(output_char)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4c01648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the 3D tensor of zero's initially\n",
    "\n",
    "# length of input_text \n",
    "# max_encoder_length is the maximum length of sentence present in input_text\n",
    "# encoder length is the length of all the unique char present in input_char\n",
    "\n",
    "encoder_input_data = np.zeros((len(input_texts),max_encoder_length,encoder_length),dtype = 'float32')\n",
    "decoder_input_data = np.zeros((len(input_texts),max_decoder_length,decoder_length),dtype = 'float32')\n",
    "decoder_output_data = np.zeros((len(input_texts),max_decoder_length,decoder_length),dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d4b75c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding pre padding to the input character \n",
    "encoder_input_data = pad_sequences(encoder_input_data,padding='pre',maxlen = max_decoder_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37d6e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning 1  where that particular char is present in the input and output Token\n",
    "for i, (input_text,target_text) in enumerate(zip(input_texts,output_texts)):\n",
    "    for t,char in enumerate(input_text):\n",
    "        encoder_input_data[i,t,input_token[char]] = 1\n",
    "        \n",
    "    encoder_input_data[i,t+1:,input_token[' ']] = 1\n",
    "        \n",
    "    for t,char_t in enumerate(target_text):\n",
    "        decoder_input_data[i,t,output_token[char_t]] = 1\n",
    "        if t>0:\n",
    "            decoder_output_data[i,t-1,output_token[char_t]] =1\n",
    "    decoder_input_data[i,t+1:,output_token[' ']]= 1\n",
    "    decoder_output_data[i,t:,output_token[' ']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b0b6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention model\n",
    "input_encoder = Input(shape = (None,encoder_length))\n",
    "\n",
    "bidirectional_lstm  = Bidirectional(LSTM(latent_dim,return_sequences =True,return_state =True))\n",
    "\n",
    "encoder_outputs1, forw_state_h, forw_state_c, back_state_h, back_state_c = bidirectional_lstm(input_encoder)\n",
    "\n",
    "final_enc_h = Concatenate()([forw_state_h,back_state_h])\n",
    "final_enc_c = Concatenate()([forw_state_c,back_state_c])\n",
    "\n",
    "# get Context vector\n",
    "encoder_states_1 =[final_enc_h, final_enc_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c860c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_decoder = Input(shape = (None,decoder_length))\n",
    "decoder_lstm = LSTM(latent_dim*2,return_sequences =True,return_state =True)\n",
    "\n",
    "decoder_outputs_1,_,_ = decoder_lstm(input_decoder,initial_state=encoder_states_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e321e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Attention Layer\n",
    "\n",
    "attention_layer = Attention()\n",
    "attention_results = attention_layer([encoder_outputs1,decoder_outputs_1])\n",
    "\n",
    "# Concat attention output and decoder LSTM output \n",
    "decoder_concat_input = Concatenate(axis=-1)([decoder_outputs_1, attention_results])\n",
    "\n",
    "decoder_dense = Dense(decoder_length,activation ='softmax')\n",
    "decoder_output_dense = decoder_dense(decoder_concat_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d63a5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([input_encoder,input_decoder],decoder_output_dense)\n",
    "model.compile(optimizer='rmsprop',loss = 'categorical_crossentropy',metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed33e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([encoder_input_data,decoder_input_data],decoder_output_data,epochs=17,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f453fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
